/*	$OpenBSD: locore.S,v 1.18 1998/09/15 10:58:53 pefo Exp $	*/
/*-
 * Copyright (c) 2016 Robert N. M. Watson
 * All rights reserved.
 *
 * This software was developed by SRI International and the University of
 * Cambridge Computer Laboratory under DARPA/AFRL contract (FA8750-10-C-0237)
 * ("CTSRD"), as part of the DARPA CRASH research programme.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 * Copyright (c) 1992, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * Digital Equipment Corporation and Ralph Campbell.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 * Copyright (C) 1989 Digital Equipment Corporation.
 * Permission to use, copy, modify, and distribute this software and
 * its documentation for any purpose and without fee is hereby granted,
 * provided that the above copyright notice appears in all copies.
 * Digital Equipment Corporation makes no representations about the
 * suitability of this software for any purpose.  It is provided "as is"
 * without express or implied warranty.
 *
 * from: Header: /sprite/src/kernel/mach/ds3100.md/RCS/loMem.s,
 *	v 1.1 89/07/11 17:55:04 nelson Exp  SPRITE (DECWRL)
 * from: Header: /sprite/src/kernel/mach/ds3100.md/RCS/machAsm.s,
 *	v 9.2 90/01/29 18:00:39 shirriff Exp  SPRITE (DECWRL)
 * from: Header: /sprite/src/kernel/vm/ds3100.md/vmPmaxAsm.s,
 *	v 1.1 89/07/10 14:27:41 nelson Exp  SPRITE (DECWRL)
 *	from: @(#)locore.s	8.5 (Berkeley) 1/4/94
 *	JNPR: exception.S,v 1.5 2007/01/08 04:58:37 katta
 * $FreeBSD$
 */

/*
 *	Contains code that is the first executed at boot time plus
 *	assembly language support routines.
 */

#include "opt_ddb.h"

#include <machine/asm.h>
#include <machine/cpu.h>
#include <machine/exceptionasm.h>
#include <machine/regnum.h>
#include <machine/cpuregs.h>
#include <machine/pte.h>
#include <machine/pcb.h>

#ifdef CPU_CHERI
#include <machine/cheriasm.h>
#include <cheri/cherireg.h>
.set cheri_sysregs_accessible	# Don't warn on access to c27-c30
#endif

#include "assym.inc"

	.set	noreorder		# Noreorder is default style!

#ifdef KDTRACE_HOOKS
	.data
	.globl	dtrace_invop_calltrap_addr
	.align	4
	.type	dtrace_invop_calltrap_addr, @object
        .size	dtrace_invop_calltrap_addr, 8
dtrace_invop_calltrap_addr:
	.word	0
	.word	0

	.text
#endif

/*
 * Reasonable limit
 */
#define	INTRCNT_COUNT	256

/*
 * General MIPS CPU state for exceptions:
 *
 * EPC Register will point to the instruction that caused fault, unless the
 * faulting instruction was in a branch delay slot.  In that case, it will
 * point to the branch before the branch delay slot instruction.
 *
 * The cause register will contain what caused the exception and some state
 * about the interrupt.
 *
 * The status register contains information about the status of the CPU such
 * as: Kernel/User mode bit, interrupt enable bit.
 *
 * The BadVaddr register contains the virtual address that cause the last
 * exception.
 *
 * The Context register contains the lower 22 bits of the VPN (starting at
 * bit 4) that cause the last exception except bit0 and bit1 are zero. The
 * upper bits (bits 23 to 31 for MIPS32 and bits 23 to 63) are set under
 * kernel control (i.e. point to the page table). The Context/XContext
 * registers are not currently used by FreeBSD.
 */

/*
 *----------------------------------------------------------------------------
 *
 * MipsTLBMiss --
 *
 *	Vector code for the TLB-miss exception vector 0x80000000.
 *
 * This code is copied to the TLB exception vector address to
 * which the CPU jumps in response to an exception or a TLB miss.
 * NOTE: This code must be position independent!!!
 *
 *
 */
VECTOR(MipsTLBMiss, unknown)
	.set push
	.set noat

#ifdef CPU_CHERI
	CHERI_EXCEPTION_ENTER(k0)
#endif
	# Increment exception counter, if enabled.
	INC_EXCEPTION_CNTR(TLB_MISS_CNT)
	j	MipsDoTLBMiss
	MFC0	k0, MIPS_COP_0_BAD_VADDR	# get the fault address
	.set pop
VECTOR_END(MipsTLBMiss)

/*
 *----------------------------------------------------------------------------
 *
 * MipsDoTLBMiss --  (UTLB miss)
 *
 * This is the real TLB Miss Handler code.  A miss was generated when the
 * access is to kuseg and there was not matching mapping loaded into the TLB.
 * 'segbase' points to the base of the segment table for user processes.
 *
 * The CPU does the following for an UTLB miss:
 * - Sets the EPC register.
 * - Sets the Cause register.
 * - Sets the Status register. Shifts K/U and IE bits over one and clears
 *   the current Kernel/User and Interrupt Enable bits. So the processor
 *   is in kernel mode with the interupts turned off.
 * - Sets BadVaddr register.
 * - Sets the Context/XContext register(s).
 * - Sets the TLB EntryHi register to contain VPN of the faulting address.
 *
 * Don't check for invalid pte's here. We load them as well and
 * let the processor trap to load the correct value after service.
 *
 * XXX This really needs to be changed to a linear page table and use the
 * Context and XContext registers.  That is really what it was designed for.
 *----------------------------------------------------------------------------
 */
 	.set push
	.set noat
MipsDoTLBMiss:
	bltz		k0, 1f				#02: k0<0 -> 1f (kernel fault)
	PTR_SRL		k0, k0, SEGSHIFT - PTRSHIFT	#03: k0=seg offset (almost)

	GET_CPU_PCPU(k1)
	PTR_L		k1, PC_SEGBASE(k1)
	beqz		k1, 2f				#05: make sure segbase is not null
	andi		k0, k0, PDEPTRMASK		#06: k0=seg offset
	PTR_ADDU	k1, k0, k1			#07: k1=seg entry address

	PTR_L		k1, 0(k1)			#08: k1=seg entry
	MFC0		k0, MIPS_COP_0_BAD_VADDR	#09: k0=bad address (again)
	beq		k1, zero, 2f			#0a: ==0 -- no page table
#ifdef __mips_n64
	PTR_SRL		k0, PDRSHIFT - PTRSHIFT		# k0=VPN
	andi		k0, k0, PDEPTRMASK		# k0=pde offset
	PTR_ADDU	k0, k0, k1			# k0=pde entry address
	PTR_L		k1, 0(k0)			# k1=pde entry
	beq		k1, zero, 2f			# ==0 -- no page table
	nop
#endif /* __mips_n64 */
	MFC0		k0, MIPS_COP_0_BAD_VADDR	# k0=bad address (again)
	PTR_SRL		k0, PAGE_SHIFT - PTESHIFT	#0b: k0=VPN (aka va>>10)
	andi		k0, k0, PTE2MASK		#0c: k0=page tab offset
	PTR_ADDU	k1, k1, k0			#0d: k1=pte address

	PTE_L		k0, 0(k1)			#0e: k0=lo0 pte

	CLEAR_PTE_SWBITS(k0)
	PTE_MTC0	k0, MIPS_COP_0_TLB_LO0		#12: lo0 is loaded
	COP0_SYNC

	PTE_L		k0, PTESIZE(k1)			# k0=lo1 pte

	CLEAR_PTE_SWBITS(k0)
	PTE_MTC0	k0, MIPS_COP_0_TLB_LO1		#15: lo1 is loaded
	COP0_SYNC
	tlbwr						#1a: write to tlb
	HAZARD_DELAY
#ifdef CPU_CHERI
	CHERI_EXCEPTION_RETURN(k0)
#endif
	eret						#1f: retUrn from exception

1:	j		MipsTLBMissException		#20: kernel exception
	nop						#21: branch delay slot
2:	j		SlowFault			#22: no page table present
	nop						#23: branch delay slot
	.set pop

/*
 * This code is copied to the general exception vector address to
 * handle all execptions except RESET and TLBMiss.
 * NOTE: This code must be position independent!!!
 */
VECTOR(MipsException, unknown)
/*
 * Find out what mode we came from and jump to the proper handler.
 *
 * Note: at turned off here because we cannot trash the at register
 * in this exception code. Only k0 and k1 may be modified before
 * we save registers. This is true of all functions called through
 * the pointer magic: Mips{User,Kern}Intr, Mips{User,Kern}GenException
 * and MipsTLBInvalidException
 */
	.set	noat
#ifdef CPU_CHERI
	CHERI_EXCEPTION_ENTER(k0)
#endif
	mfc0	k0, MIPS_COP_0_STATUS		# Get the status register
	mfc0	k1, MIPS_COP_0_CAUSE		# Get the cause register value.
	and	k0, k0, MIPS_SR_KSU_USER	# test for user mode
						# sneaky but the bits are
						# with us........
	sll	k0, k0, 3			# shift user bit for cause index
	and	k1, k1, MIPS_CR_EXC_CODE	# Mask out the cause bits.
	or	k1, k1, k0			# change index to user table
#if defined(__mips_n64)
	PTR_SLL	k1, k1, 1			# shift to get 8-byte offset
#endif
1:
	PTR_LA	k0, _C_LABEL(machExceptionTable)  # get base of the jump table
	PTR_ADDU k0, k0, k1			# Get the address of the
						#  function entry.  Note that
						#  the cause is already
						#  shifted left by 2 bits so
						#  we dont have to shift.
	PTR_L	k0, 0(k0)			# Get the function address
	nop
	j	k0				# Jump to the function.
	nop
	.set	at
VECTOR_END(MipsException)

/*
 * We couldn't find a TLB entry.
 * Find out what mode we came from and call the appropriate handler.
 */
SlowFault:
	.set	noat
	mfc0	k0, MIPS_COP_0_STATUS
	nop
	and	k0, k0, MIPS_SR_KSU_USER
	bne	k0, zero, _C_LABEL(MipsUserGenException)
	nop
	.set	at
/*
 * Fall though ...
 */

/*----------------------------------------------------------------------------
 *
 * MipsKernGenException --
 *
 *	Handle an exception from kernel mode.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------------
 */

/*
 * The kernel exception stack contains 40 general-purpose registers, hi/lo
 * registers, and status register.  If CHERI is present, a further 33 CHERI
 * registers are stored there.  We also set up linkage conventions.  The
 * on-stack frame must match the binary layout of 'struct trapframe'.
 *
 * If we store capability registers on the stack, we need to adjust the stack
 * pointer to provide suitable alignment.  We therefore allocate an additional
 * CHERICAP_SIZE/2 to allow for that adjustment.  a0 will be set suitably to
 * include that alignment.
 */
#define	KERN_REG_SIZE		(NUMSAVEREGS * SZREG)
#ifdef CPU_CHERI
#define	KERN_CREG_SIZE		(NUMCHERISAVEREGS * CHERICAP_SIZE)
#define	KERN_EXC_FRAME_SIZE	(CALLFRAME_SIZ + KERN_REG_SIZE +	\
				KERN_CREG_SIZE)
#else
#define	KERN_EXC_FRAME_SIZE	(CALLFRAME_SIZ + KERN_REG_SIZE)
#endif

#ifdef CPU_CHERI
/*
 * CHERI requires 16- or 32-byte alignment of the trap frame, so adjust sp
 * (or, whatever register is passed in via 'reg') down if required.  The
 * previous value of sp should be saved prior to calling this macro --
 * typically, in k1.  There are no free registers, so we do this using shift
 * instructions.
 */
#define	CHERI_ADJUST_SP(reg)						\
	dsrl	reg, reg, CHERICAP_SHIFT;				\
	dsll	reg, reg, CHERICAP_SHIFT
#else
#define	CHERI_ADJUST_SP(reg)
#endif

#ifdef CPU_CHERI
/*
 * Save CHERI registers on the stack to construct the kernel trap frame.
 * Suitable alignment should already have been arranged by the caller.
 */
#define	SAVE_CREG(creg, regoffs, offs, cregbase)	\
	csc	creg, regoffs, (CALLFRAME_SIZ + (SZREG * offs))(cregbase)

#define	SAVE_CAPCAUSE(reg, treg, regoffs, offs, cregbase)		\
	daddiu	treg, regoffs, (CALLFRAME_SIZ + (SZREG * offs));	\
	csd	reg, treg, 0(cregbase)

#define	SAVE_CHERI(treg0, treg1)				\
	SAVE_CREG(CHERI_REG_C1, sp, C1, $ddc);		\
	SAVE_CREG(CHERI_REG_C2, sp, C2, $ddc);		\
	SAVE_CREG(CHERI_REG_C3, sp, C3, $ddc);		\
	SAVE_CREG(CHERI_REG_C4, sp, C4, $ddc);		\
	SAVE_CREG(CHERI_REG_C5, sp, C5, $ddc);		\
	SAVE_CREG(CHERI_REG_C6, sp, C6, $ddc);		\
	SAVE_CREG(CHERI_REG_C7, sp, C7, $ddc);		\
	SAVE_CREG(CHERI_REG_C8, sp, C8, $ddc);		\
	SAVE_CREG(CHERI_REG_C9, sp, C9, $ddc);		\
	SAVE_CREG(CHERI_REG_C10, sp, C10, $ddc);	\
	SAVE_CREG(CHERI_REG_STC, sp, STC, $ddc);	\
	SAVE_CREG(CHERI_REG_C12, sp, C12, $ddc);	\
	SAVE_CREG(CHERI_REG_C13, sp, C13, $ddc);	\
	SAVE_CREG(CHERI_REG_C14, sp, C14, $ddc);	\
	SAVE_CREG(CHERI_REG_C15, sp, C15, $ddc);	\
	SAVE_CREG(CHERI_REG_C16, sp, C16, $ddc);	\
	SAVE_CREG(CHERI_REG_C17, sp, C17, $ddc);	\
	SAVE_CREG(CHERI_REG_C18, sp, C18, $ddc);	\
	SAVE_CREG(CHERI_REG_C19, sp, C19, $ddc);	\
	SAVE_CREG(CHERI_REG_C20, sp, C20, $ddc);	\
	SAVE_CREG(CHERI_REG_C21, sp, C21, $ddc);	\
	SAVE_CREG(CHERI_REG_C22, sp, C22, $ddc);	\
	SAVE_CREG(CHERI_REG_C23, sp, C23, $ddc);	\
	SAVE_CREG(CHERI_REG_C24, sp, C24, $ddc);	\
	SAVE_CREG(CHERI_REG_C25, sp, C25, $ddc);	\
	SAVE_CREG(CHERI_REG_C26, sp, IDC, $ddc);	\
	SAVE_CREG(CHERI_REG_C27, sp, C27, $ddc);	\
	SAVE_CREG(CHERI_REG_C28, sp, C28, $ddc);	\
	SAVE_CREG(CHERI_REG_C29, sp, C29, $ddc);	\
	SAVE_CREG(CHERI_REG_C30, sp, C30, $ddc);	\
	SAVE_CREG(CHERI_REG_C31, sp, C31, $ddc);	\
	/* Save special registers after KSCRATCH (C27) */	\
	CGetDefault CHERI_REG_KSCRATCH;			\
	SAVE_CREG(CHERI_REG_KSCRATCH, sp, DDC, $ddc);	\
	cgetcause	treg0;				\
	SAVE_CAPCAUSE(treg0, treg1, sp, CAPCAUSE, $ddc);	\
	/* Save $epcc last to keep it in KSCRATCH */	\
	CGetEPCC CHERI_REG_KSCRATCH;			\
	SAVE_CREG(CHERI_REG_KSCRATCH, sp, PCC, $ddc)
#else
#define	SAVE_CHERI(treg0, treg1)
#endif

/*
 * Save CPU and CP0 register state when taking an exception in kernel mode.
 * The caller will already have set up a stack pointer with suitable space and
 * alignment.
 *
 * This is straightforward except for saving the exception program
 * counter. The ddb backtrace code looks for the first instruction
 * matching the form "sw ra, (off)sp" to figure out the address of the
 * calling function. So we must make sure that we save the exception
 * PC by staging it through 'ra' as opposed to any other register.
 *
 * sp passes in the pointer to where we should place the call frame and trap
 * frame on the stack.
 * k1 passes in the preempted stack pointer (to be saved at SP).
 * a0 returns a pointer to the trap frame.
 * k0 is used as a temporary to hold the CP0 status register.
 *
 * sp and k1 may differ if we've had to re-align the stack for CHERI.
 */
#define	SAVE_REG(reg, offs, base) \
	REG_S	reg, (CALLFRAME_SIZ + (SZREG * offs)) (base)

#if __has_feature(capabilities)
/*
 * Note: for CHERI the save is done by SAVE_CHERI() but we still need to move
 * the address of $epcc into addr_gpr
 */
#define SAVE_EPC_REG(addr_gpr, epcc_reg, base)	\
	CGetEPCC epcc_reg;			\
	CGetAddr addr_gpr, epcc_reg
#else
#define SAVE_EPC_REG(addr_gpr, epcc_reg, base)	\
	MFC0	addr_gpr, MIPS_COP_0_EXC_PC;	\
	SAVE_REG(addr_gpr, PC, base)
#endif


#define	SAVE_CPU \
	SAVE_REG(AT, AST, sp)		;\
	.set	at		        ;\
	SAVE_REG(v0, V0, sp)		;\
	SAVE_REG(v1, V1, sp)		;\
	SAVE_REG(a0, A0, sp)		;\
	SAVE_REG(a1, A1, sp)		;\
	SAVE_REG(a2, A2, sp)		;\
	SAVE_REG(a3, A3, sp)		;\
	SAVE_REG(t0, T0, sp)		;\
	SAVE_REG(t1, T1, sp)		;\
	SAVE_REG(t2, T2, sp)		;\
	SAVE_REG(t3, T3, sp)		;\
	SAVE_REG(ta0, TA0, sp)		;\
	SAVE_REG(ta1, TA1, sp)		;\
	SAVE_REG(ta2, TA2, sp)		;\
	SAVE_REG(ta3, TA3, sp)		;\
	SAVE_REG(t8, T8, sp)		;\
	SAVE_REG(t9, T9, sp)		;\
	SAVE_REG(gp, GP, sp)		;\
	SAVE_REG(s0, S0, sp)		;\
	SAVE_REG(s1, S1, sp)		;\
	SAVE_REG(s2, S2, sp)		;\
	SAVE_REG(s3, S3, sp)		;\
	SAVE_REG(s4, S4, sp)		;\
	SAVE_REG(s5, S5, sp)		;\
	SAVE_REG(s6, S6, sp)		;\
	SAVE_REG(s7, S7, sp)		;\
	SAVE_REG(s8, S8, sp)	        ;\
	mflo	v0			;\
	mfhi	v1			;\
	mfc0	a0, MIPS_COP_0_STATUS	;\
	mfc0	a1, MIPS_COP_0_CAUSE	;\
	MFC0	a2, MIPS_COP_0_BAD_VADDR;\
	SAVE_REG(v0, MULLO, sp)		;\
	SAVE_REG(v1, MULHI, sp)		;\
	SAVE_REG(a0, SR, sp)		;\
	SAVE_REG(a1, CAUSE, sp)		;\
	SAVE_REG(a2, BADVADDR, sp)	;\
	/* SAVE_CHERI saves $epcc */	;\
	SAVE_CHERI(t0, t1)		;\
	SAVE_EPC_REG(a3, CHERI_REG_KSCRATCH, sp);\
	SAVE_REG(ra, RA, sp)		;\
	SAVE_REG(k1, SP, sp)		/* Notice: k1 stored for sp. */	;\
	CLEAR_STATUS			;\
	PTR_ADDU a0, sp, CALLFRAME_SIZ	/* Make trap frame pointer. */	;\
	ITLBNOPFIX

#ifdef CPU_CHERI
#define	RESTORE_CREG(creg, regoffs, offs, cregbase)	\
	clc	creg, regoffs, (CALLFRAME_SIZ + (SZREG * offs))(cregbase)

/*
 * Restore CHERI registers from the on-stack kernel trap frame.
 *
 * Notice: the capability cause register is saved, but not restored.
 */
#define	RESTORE_CHERI						\
	/* Restore special registers before KSCRATCH (C27) */	\
	RESTORE_CREG(CHERI_REG_KSCRATCH, sp, DDC, $ddc);	\
	CSetDefault CHERI_REG_KSCRATCH;		\
	RESTORE_CREG(CHERI_REG_C1, sp, C1, $ddc);	\
	RESTORE_CREG(CHERI_REG_C2, sp, C2, $ddc);	\
	RESTORE_CREG(CHERI_REG_C3, sp, C3, $ddc);	\
	RESTORE_CREG(CHERI_REG_C4, sp, C4, $ddc);	\
	RESTORE_CREG(CHERI_REG_C5, sp, C5, $ddc);	\
	RESTORE_CREG(CHERI_REG_C6, sp, C6, $ddc);	\
	RESTORE_CREG(CHERI_REG_C7, sp, C7, $ddc);	\
	RESTORE_CREG(CHERI_REG_C8, sp, C8, $ddc);	\
	RESTORE_CREG(CHERI_REG_C9, sp, C9, $ddc);	\
	RESTORE_CREG(CHERI_REG_C10, sp, C10, $ddc);	\
	RESTORE_CREG(CHERI_REG_STC, sp, STC, $ddc);	\
	RESTORE_CREG(CHERI_REG_C12, sp, C12, $ddc);	\
	RESTORE_CREG(CHERI_REG_C13, sp, C13, $ddc);	\
	RESTORE_CREG(CHERI_REG_C14, sp, C14, $ddc);	\
	RESTORE_CREG(CHERI_REG_C15, sp, C15, $ddc);	\
	RESTORE_CREG(CHERI_REG_C16, sp, C16, $ddc);	\
	RESTORE_CREG(CHERI_REG_C17, sp, C17, $ddc);	\
	RESTORE_CREG(CHERI_REG_C18, sp, C18, $ddc);	\
	RESTORE_CREG(CHERI_REG_C19, sp, C19, $ddc);	\
	RESTORE_CREG(CHERI_REG_C20, sp, C20, $ddc);	\
	RESTORE_CREG(CHERI_REG_C21, sp, C21, $ddc);	\
	RESTORE_CREG(CHERI_REG_C22, sp, C22, $ddc);	\
	RESTORE_CREG(CHERI_REG_C23, sp, C23, $ddc);	\
	RESTORE_CREG(CHERI_REG_C24, sp, C24, $ddc);	\
	RESTORE_CREG(CHERI_REG_C25, sp, C25, $ddc);	\
	RESTORE_CREG(CHERI_REG_C26, sp, IDC, $ddc);	\
	RESTORE_CREG(CHERI_REG_C27, sp, C27, $ddc);	\
	RESTORE_CREG(CHERI_REG_C28, sp, C28, $ddc);	\
	RESTORE_CREG(CHERI_REG_C29, sp, C29, $ddc);	\
	RESTORE_CREG(CHERI_REG_C30, sp, C30, $ddc);	\
	RESTORE_CREG(CHERI_REG_C31, sp, C31, $ddc);	\

/* For CHERI we don't write to EPC since it is already updated by CSetEPCC. */
#define MTC0_EXC_PC(epc, epcc) CSetEPCC epcc;
#else
#define	RESTORE_CHERI
#define MTC0_EXC_PC(epc, epcc) MTC0	epc, MIPS_COP_0_EXC_PC
#endif

/*
 * Restore preempted kernel state following a kernel exception.  The caller
 * will restore the original sp from k1 after RESTORE_CPU has ended.
 */
#define	RESTORE_REG(reg, offs, base) \
	REG_L	reg, (CALLFRAME_SIZ + (SZREG * offs)) (base)

#define	RESTORE_CPU \
	CLEAR_STATUS			;\
	RESTORE_REG(k0, SR, sp)		;\
	RESTORE_REG(t0, MULLO, sp)	;\
	RESTORE_REG(t1, MULHI, sp)	;\
	mtlo	t0			;\
	mthi	t1			;\
	MTC0_EXC_PC(v0, $c3)		;\
	/* Now that $c3 has been moved	\
	 * to $epcc, we can restore the	\
	 * remaining CHERI registers */	\
	RESTORE_CHERI			;\
	.set noat		        ;\
	RESTORE_REG(AT, AST, sp)	;\
	RESTORE_REG(v0, V0, sp)		;\
	RESTORE_REG(v1, V1, sp)		;\
	RESTORE_REG(a0, A0, sp)		;\
	RESTORE_REG(a1, A1, sp)		;\
	RESTORE_REG(a2, A2, sp)		;\
	RESTORE_REG(a3, A3, sp)		;\
	RESTORE_REG(t0, T0, sp)		;\
	RESTORE_REG(t1, T1, sp)		;\
	RESTORE_REG(t2, T2, sp)		;\
	RESTORE_REG(t3, T3, sp)		;\
	RESTORE_REG(ta0, TA0, sp)	;\
	RESTORE_REG(ta1, TA1, sp)	;\
	RESTORE_REG(ta2, TA2, sp)	;\
	RESTORE_REG(ta3, TA3, sp)	;\
	RESTORE_REG(t8, T8, sp)		;\
	RESTORE_REG(t9, T9, sp)		;\
	RESTORE_REG(s0, S0, sp)		;\
	RESTORE_REG(s1, S1, sp)		;\
	RESTORE_REG(s2, S2, sp)		;\
	RESTORE_REG(s3, S3, sp)		;\
	RESTORE_REG(s4, S4, sp)		;\
	RESTORE_REG(s5, S5, sp)		;\
	RESTORE_REG(s6, S6, sp)		;\
	RESTORE_REG(s7, S7, sp)		;\
	RESTORE_REG(s8, S8, sp)	        ;\
	RESTORE_REG(gp, GP, sp)		;\
	RESTORE_REG(ra, RA, sp)		;\
	RESTORE_REG(k1, SP, sp)		/* Notice: restored sp in k1. */ ;\
	mtc0	k0, MIPS_COP_0_STATUS

NESTED_NOPROFILE(MipsKernGenException, KERN_EXC_FRAME_SIZE, ra)
	.set	noat

	/* Save exception sp in k1 to put on the stack later. */
	move	k1, sp
	PTR_SUBU	sp, sp, KERN_EXC_FRAME_SIZE
	CHERI_ADJUST_SP(sp)
	.mask	0x80000000, (CALLFRAME_RA - KERN_EXC_FRAME_SIZE)
/*
 * Save CPU state, building 'frame'.  sp holds the location to save the trap
 * frame, whereas k1 holds the value of sp to save in the trap frame.
 */
	SAVE_CPU
/*
 * Call the exception handler.  SAVE_CPU has left a0 pointing at the saved
 * frame.
 */
	PTR_LA	gp, _C_LABEL(_gp)
	PTR_LA	t9, _C_LABEL(trap)
	jalr	t9
	REG_S	a3, (CALLFRAME_RA + KERN_REG_SIZE)(sp)		# for debugging

	/*
	 * Update interrupt and CPU mask in saved status register
	 * Some of interrupts could be disabled by
	 * intr filters if interrupts are enabled later
	 * in trap handler
	 */
	mfc0	a0, MIPS_COP_0_STATUS
	and	a0, a0, (MIPS_SR_INT_MASK|MIPS_SR_COP_USABILITY)
	RESTORE_REG(a1, SR, sp)
	and	a1, a1, ~(MIPS_SR_INT_MASK|MIPS_SR_COP_USABILITY)
	or	a1, a1, a0
	SAVE_REG(a1, SR, sp)
	RESTORE_CPU			# v0/$c3 contains the return address.

	/* Restore preempted sp from k1 now we are done with the stack. */
	move	sp, k1
	sync

#ifdef CPU_CHERI
	CHERI_EXCEPTION_RETURN(k0)
#endif
	eret
	.set	at
END(MipsKernGenException)


/*----------------------------------------------------------------------------
 *
 * MipsUserGenException --
 *
 *	Handle an exception from user mode.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------------
 */
NESTED_NOPROFILE(MipsUserGenException, CALLFRAME_SIZ, ra)
	.set	noat
	.mask	0x80000000, (CALLFRAME_RA - CALLFRAME_SIZ)
	GET_CPU_PCPU(k1)
	PTR_L	k1, PC_CURPCB(k1)
	SAVE_REGS_TO_PCB(k1)
#ifdef CPU_CHERI
	/*
	 * Note: This saves EPCC, matching the explicit EPC save above.
	 */
	SAVE_CREGS_TO_PCB(k1, t0)
#endif
	REG_S	a3, CALLFRAME_RA(sp)	# for debugging
	PTR_LA	gp, _C_LABEL(_gp)	# switch to kernel GP
# Turn off fpu and enter kernel mode
	and	t0, a0, ~(MIPS_SR_COP_1_BIT | MIPS_SR_EXL | MIPS_SR_KSU_MASK | MIPS_SR_INT_IE)
#if defined(CPU_CNMIPS)
	and	t0, t0, ~(MIPS_SR_COP_2_BIT)
	or      t0, t0, (MIPS_SR_KX | MIPS_SR_SX | MIPS_SR_UX | MIPS_SR_PX)
#elif defined(CPU_RMI)	|| defined(CPU_NLM)
	or      t0, t0, (MIPS_SR_KX | MIPS_SR_UX | MIPS_SR_COP_2_BIT)
#endif	
	mtc0	t0, MIPS_COP_0_STATUS

	PTR_ADDU a0, k1, U_PCB_REGS
	ITLBNOPFIX

/*
 * Call the exception handler. Return pc is in $c3 for CHERI and $v0 for MIPS.
 * However, that is currently ignore since it is the same as trapframe->pc.
 */
	PTR_LA	t9, _C_LABEL(trap)
	jalr	t9
	nop

/*
 * Restore user registers and return.
 * First disable interrupts and set exeption level.
 */
	DO_AST

	CLEAR_STATUS

/*
 * The use of k1 for storing the PCB pointer must be done only
 * after interrupts are disabled.  Otherwise it will get overwritten
 * by the interrupt code.
 */
	GET_CPU_PCPU(k1)
	PTR_L	k1, PC_CURPCB(k1)

	/*
	 * Update interrupt mask in saved status register
	 * Some of interrupts could be enabled by ithread
	 * scheduled by ast()
	 */
	mfc0	a0, MIPS_COP_0_STATUS
	and	a0, a0, MIPS_SR_INT_MASK
	RESTORE_U_PCB_REG(a1, SR, k1)
	and	a1, a1, ~MIPS_SR_INT_MASK
	or	a1, a1, a0
	SAVE_U_PCB_REG(a1, SR, k1)

#ifdef CPU_CHERI
	/*
	 * Note: This restores EPCC (RESTORE_REGS_FROM_PCB does not touch EPC)
	 */
	RESTORE_CREGS_FROM_PCB(k1, t0)
#endif
	RESTORE_REGS_FROM_PCB(k1)

	mtc0	k0, MIPS_COP_0_STATUS	# still exception level
	ITLBNOPFIX
	sync
#ifdef CPU_CHERI
	CHERI_EXCEPTION_RETURN(k0)
#endif
	eret
	.set	at
END(MipsUserGenException)

	.set	push
	.set	noat
NESTED(mips_wait, CALLFRAME_SIZ, ra)
	PTR_SUBU        sp, sp, CALLFRAME_SIZ
	.mask   0x80000000, (CALLFRAME_RA - CALLFRAME_SIZ)
	REG_S   ra, CALLFRAME_RA(sp)		# save RA
	mfc0	t0, MIPS_COP_0_STATUS
	xori	t1, t0, MIPS_SR_INT_IE
	mtc0	t1, MIPS_COP_0_STATUS
	COP0_SYNC
	PTR_LA	t9, sched_runnable
	jalr	t9
	nop
	REG_L   ra, CALLFRAME_RA(sp)
	mfc0	t0, MIPS_COP_0_STATUS
	ori	t1, t0, MIPS_SR_INT_IE
	.align 4
GLOBAL(MipsWaitStart)			# this is 16 byte aligned
	mtc0	t1, MIPS_COP_0_STATUS
	bnez	v0, MipsWaitEnd
	nop
#if defined(CPU_XBURST) && defined(SMP)
	nop
#else
	wait
#endif
GLOBAL(MipsWaitEnd)			# MipsWaitStart + 16
	jr	ra
	PTR_ADDU        sp, sp, CALLFRAME_SIZ
END(mips_wait)
	.set	pop

/*----------------------------------------------------------------------------
 *
 * MipsKernIntr --
 *
 *	Handle an interrupt from kernel mode.
 *	Interrupts use the standard kernel stack.
 *	switch_exit sets up a kernel stack after exit so interrupts won't fail.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------------
 */

NESTED_NOPROFILE(MipsKernIntr, KERN_EXC_FRAME_SIZE, ra)
	.set	noat

/*
 * Check for getting interrupts just before wait
 *
 * XXXCHERI: Once we use variable CHERI PCC in the kernel, this check will
 * also need to take that into account.  In the mean time, the fact that we're
 * in the kernel ring is sufficient to imply that PCC matches the kernel
 * address space.
 */
#if __has_feature(capabilities)
	/* Save previous $c27. */
	csetkr1c	CHERI_REG_KSCRATCH
	CGetEPCC CHERI_REG_KSCRATCH
	/* Get the address (not offset!) of $epcc to perform the range check */
	CGetAddr	k0, CHERI_REG_KSCRATCH
#else
	MFC0	k0, MIPS_COP_0_EXC_PC
#endif
	ori	k0, 0xf
	xori	k0, 0xf			# 16 byte align
	PTR_LA	k1, MipsWaitStart
	bne	k0, k1, 1f
	nop
	PTR_ADDU k1, 16			# skip over wait
#if __has_feature(capabilities)
	CSetAddr	CHERI_REG_KSCRATCH, CHERI_REG_KSCRATCH, k1
	CSetEPCC	CHERI_REG_KSCRATCH
#else
	MTC0	k1, MIPS_COP_0_EXC_PC
#endif
1:

#if __has_feature(capabilities)
	/* Restore previous $c27. */
	cgetkr1c	CHERI_REG_KSCRATCH
#endif

	/* Save exception sp in k1 to put on the stack later. */
	move	k1, sp
	PTR_SUBU	sp, sp, KERN_EXC_FRAME_SIZE
	CHERI_ADJUST_SP(sp)
	.mask	0x80000000, (CALLFRAME_RA - KERN_EXC_FRAME_SIZE)

/*
 * Save CPU state, building 'frame'.  sp holds the location to save the trap
 * frame, whereas k1 holds the value of sp to save in the trap frame.
 */
	SAVE_CPU
/*
 * Call the interrupt handler.   SAVE_CPU has left a0 pointing at the saved
 * frame.
 */
	PTR_LA	gp, _C_LABEL(_gp)
#ifdef INTRNG
	PTR_LA	t9, _C_LABEL(intr_irq_handler)
#else
	PTR_LA	t9, _C_LABEL(cpu_intr)
#endif
	jalr	t9
	# SAVE_CPU places EPC into a3. For CHERI we don't save EPC (the offset
	# of $epcc) but instad a3 will contain the address of $epcc.
	REG_S	a3, CALLFRAME_RA + KERN_REG_SIZE(sp)

	/*
	 * Update interrupt and CPU mask in saved status register
	 * Some of interrupts could be disabled by
	 * intr filters if interrupts are enabled later
	 * in trap handler
	 */
	mfc0	a0, MIPS_COP_0_STATUS
	and	a0, a0, (MIPS_SR_INT_MASK|MIPS_SR_COP_USABILITY)
	RESTORE_REG(a1, SR, sp)
	and	a1, a1, ~(MIPS_SR_INT_MASK|MIPS_SR_COP_USABILITY)
	or	a1, a1, a0
	SAVE_REG(a1, SR, sp)
	REG_L	v0, (CALLFRAME_RA + KERN_REG_SIZE)(sp)
#if __has_feature(capabilities)
	/* v0 contains the target virtual address -> set it on saved $epcc */
	/* XXX: This will need changing for purecap */
	RESTORE_CREG($c3, sp, PCC, $ddc)
	CSetAddr	$c3, $c3, v0
#endif
	RESTORE_CPU			# v0/$c3 contains the return address.

	/* Restore preempted sp from k1 now we are done with the stack. */
	move	sp, k1
	sync
#ifdef CPU_CHERI
	CHERI_EXCEPTION_RETURN(k0)
#endif
	eret
	.set	at
END(MipsKernIntr)

/*----------------------------------------------------------------------------
 *
 * MipsUserIntr --
 *
 *	Handle an interrupt from user mode.
 *	Note: we save minimal state in the u.u_pcb struct and use the standard
 *	kernel stack since there has to be a u page if we came from user mode.
 *	If there is a pending software interrupt, then save the remaining state
 *	and call softintr(). This is all because if we call switch() inside
 *	interrupt(), not all the user registers have been saved in u.u_pcb.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------------
 */
NESTED_NOPROFILE(MipsUserIntr, CALLFRAME_SIZ, ra)
	.set	noat
	.mask	0x80000000, (CALLFRAME_RA - CALLFRAME_SIZ)
/*
 * Save the relevant user registers into the u.u_pcb struct.
 * We don't need to save s0 - s8 because the compiler does it for us.
 */
	GET_CPU_PCPU(k1)
	PTR_L	k1, PC_CURPCB(k1)
	SAVE_U_PCB_REG(AT, AST, k1)
	.set	at
	SAVE_U_PCB_REG(v0, V0, k1)
	SAVE_U_PCB_REG(v1, V1, k1)
	SAVE_U_PCB_REG(a0, A0, k1)
	SAVE_U_PCB_REG(a1, A1, k1)
	SAVE_U_PCB_REG(a2, A2, k1)
	SAVE_U_PCB_REG(a3, A3, k1)
	SAVE_U_PCB_REG(t0, T0, k1)
	SAVE_U_PCB_REG(t1, T1, k1)
	SAVE_U_PCB_REG(t2, T2, k1)
	SAVE_U_PCB_REG(t3, T3, k1)
	SAVE_U_PCB_REG(ta0, TA0, k1)
	SAVE_U_PCB_REG(ta1, TA1, k1)
	SAVE_U_PCB_REG(ta2, TA2, k1)
	SAVE_U_PCB_REG(ta3, TA3, k1)
	SAVE_U_PCB_REG(t8, T8, k1)
	SAVE_U_PCB_REG(t9, T9, k1)
	SAVE_U_PCB_REG(gp, GP, k1)
	SAVE_U_PCB_REG(sp, SP, k1)
	SAVE_U_PCB_REG(ra, RA, k1)
/*
 *  save remaining user state in u.u_pcb.
 */
	SAVE_U_PCB_REG(s0, S0, k1)
	SAVE_U_PCB_REG(s1, S1, k1)
	SAVE_U_PCB_REG(s2, S2, k1)
	SAVE_U_PCB_REG(s3, S3, k1)
	SAVE_U_PCB_REG(s4, S4, k1)
	SAVE_U_PCB_REG(s5, S5, k1)
	SAVE_U_PCB_REG(s6, S6, k1)
	SAVE_U_PCB_REG(s7, S7, k1)
	SAVE_U_PCB_REG(s8, S8, k1)

	mflo	v0			# get lo/hi late to avoid stall
	mfhi	v1
	mfc0	a0, MIPS_COP_0_STATUS
	mfc0	a1, MIPS_COP_0_CAUSE
	SAVE_U_PCB_REG(v0, MULLO, k1)
	SAVE_U_PCB_REG(v1, MULHI, k1)
	SAVE_U_PCB_REG(a0, SR, k1)
	SAVE_U_PCB_REG(a1, CAUSE, k1)
	SAVE_U_PCB_EPC(a3, k1)	# $pc in a3, note used later! (not for CHERI)
#if __has_feature(capabilities)
	/*
	 * Note: This saves EPCC, and leaves it in CHERI_REG_KSCRATCH
	 * so we can do a CGetAddr to get the pc vaddr in a3 to match MIPS
	 */
	SAVE_CREGS_TO_PCB(k1, t0)
	CGetAddr	a3, CHERI_REG_KSCRATCH
#endif
	PTR_SUBU	sp, k1, CALLFRAME_SIZ  # switch to kernel SP
	PTR_LA	gp, _C_LABEL(_gp)	# switch to kernel GP

# Turn off fpu, disable interrupts, set kernel mode kernel mode, clear exception level.
	and	t0, a0, ~(MIPS_SR_COP_1_BIT | MIPS_SR_EXL | MIPS_SR_INT_IE | MIPS_SR_KSU_MASK)
#ifdef CPU_CNMIPS
	and	t0, t0, ~(MIPS_SR_COP_2_BIT)
	or      t0, t0, (MIPS_SR_KX | MIPS_SR_SX | MIPS_SR_UX | MIPS_SR_PX)
#elif defined(CPU_RMI)	|| defined(CPU_NLM)
	or      t0, t0, (MIPS_SR_KX | MIPS_SR_UX | MIPS_SR_COP_2_BIT)
#endif	
	mtc0	t0, MIPS_COP_0_STATUS
	ITLBNOPFIX
	PTR_ADDU a0, k1, U_PCB_REGS
/*
 * Call the interrupt handler.
 */
#ifdef INTRNG
	PTR_LA	t9, _C_LABEL(intr_irq_handler)
#else
	PTR_LA	t9, _C_LABEL(cpu_intr)
#endif
	jalr	t9
	REG_S	a3, CALLFRAME_RA(sp)	# for debugging

/*
 * Enable interrupts before doing ast().
 *
 * On SMP kernels the AST processing might trigger IPI to other processors.
 * If that processor is also doing AST processing with interrupts disabled
 * then we may deadlock.
 */
	mfc0	a0, MIPS_COP_0_STATUS
	or	a0, a0, MIPS_SR_INT_IE
	mtc0	a0, MIPS_COP_0_STATUS
	ITLBNOPFIX

/*
 * DO_AST enabled interrupts
 */
	DO_AST
	
/*
 * Restore user registers and return. 
 */
 	CLEAR_STATUS

	GET_CPU_PCPU(k1)
	PTR_L	k1, PC_CURPCB(k1)

	/*
	 * Update interrupt mask in saved status register
	 * Some of interrupts could be disabled by
	 * intr filters
	 */
	mfc0	a0, MIPS_COP_0_STATUS
	and	a0, a0, MIPS_SR_INT_MASK
	RESTORE_U_PCB_REG(a1, SR, k1)
	and	a1, a1, ~MIPS_SR_INT_MASK
	or	a1, a1, a0
	SAVE_U_PCB_REG(a1, SR, k1)

#ifdef CPU_CHERI
	/*
	 * Note: This does not restore EPCC, since that is restored by
	 * RESTORE_U_PCB_PC() below.
	 */
	RESTORE_CREGS_FROM_PCB(k1, t0)
#endif
	RESTORE_U_PCB_REG(s0, S0, k1)
	RESTORE_U_PCB_REG(s1, S1, k1)
	RESTORE_U_PCB_REG(s2, S2, k1)
	RESTORE_U_PCB_REG(s3, S3, k1)
	RESTORE_U_PCB_REG(s4, S4, k1)
	RESTORE_U_PCB_REG(s5, S5, k1)
	RESTORE_U_PCB_REG(s6, S6, k1)
	RESTORE_U_PCB_REG(s7, S7, k1)
	RESTORE_U_PCB_REG(s8, S8, k1)
	RESTORE_U_PCB_REG(t0, MULLO, k1)
	RESTORE_U_PCB_REG(t1, MULHI, k1)
	mtlo	t0
	mthi	t1
	RESTORE_U_PCB_PC(t0, k1)	# set return address
	RESTORE_U_PCB_REG(v0, V0, k1)
	RESTORE_U_PCB_REG(v1, V1, k1)
	RESTORE_U_PCB_REG(a0, A0, k1)
	RESTORE_U_PCB_REG(a1, A1, k1)
	RESTORE_U_PCB_REG(a2, A2, k1)
	RESTORE_U_PCB_REG(a3, A3, k1)
	RESTORE_U_PCB_REG(t0, T0, k1)
	RESTORE_U_PCB_REG(t1, T1, k1)
	RESTORE_U_PCB_REG(t2, T2, k1)
	RESTORE_U_PCB_REG(t3, T3, k1)
	RESTORE_U_PCB_REG(ta0, TA0, k1)
	RESTORE_U_PCB_REG(ta1, TA1, k1)
	RESTORE_U_PCB_REG(ta2, TA2, k1)
	RESTORE_U_PCB_REG(ta3, TA3, k1)
	RESTORE_U_PCB_REG(t8, T8, k1)
	RESTORE_U_PCB_REG(t9, T9, k1)
	RESTORE_U_PCB_REG(gp, GP, k1)
	RESTORE_U_PCB_REG(k0, SR, k1)
	RESTORE_U_PCB_REG(sp, SP, k1)
	RESTORE_U_PCB_REG(ra, RA, k1)
	.set	noat
	RESTORE_U_PCB_REG(AT, AST, k1)

	mtc0	k0, MIPS_COP_0_STATUS	# SR with EXL set. 
	ITLBNOPFIX
	sync
#ifdef CPU_CHERI
	CHERI_EXCEPTION_RETURN(k0)
#endif
	eret
	.set	at
END(MipsUserIntr)

#if defined(MIPS_EXC_CNTRS)
/* A stub for counting TLB modification exceptions. */
LEAF_NOPROFILE(MipsTLBModException)
	.set push
	.set noat

	# Increment exception counter, if enabled.
	INC_EXCEPTION_CNTR(TLB_MOD_CNT)
	j	MipsKernGenException
	nop
	.set pop
END(MipsTLBModException)
#endif /* defined(MIPS_EXC_CNTRS) */

LEAF_NOPROFILE(MipsTLBInvalidException)
	.set push
	.set noat
	.set noreorder

	# Increment exception counter, if enabled.
	INC_EXCEPTION_CNTR(TLB_INVALID_CNT)
	MFC0		k0, MIPS_COP_0_BAD_VADDR
	PTR_LI		k1, VM_MAXUSER_ADDRESS
	sltu		k1, k0, k1
	bnez		k1, 1f
	nop

	/* Kernel address.  */
	lui		k1, %hi(kernel_segmap)		# k1=hi of segbase
	b		2f
	PTR_L		k1, %lo(kernel_segmap)(k1)	# k1=segment tab base

1:	/* User address.  */
	GET_CPU_PCPU(k1)
	PTR_L		k1, PC_SEGBASE(k1)

2:	/* Validate page directory pointer.  */
	beqz		k1, 3f
	nop

	PTR_SRL		k0, SEGSHIFT - PTRSHIFT		# k0=seg offset (almost)
	beq		k1, zero, MipsKernGenException	# ==0 -- no seg tab
	andi		k0, k0, PDEPTRMASK		#06: k0=seg offset
	PTR_ADDU	k1, k0, k1			# k1=seg entry address
	PTR_L		k1, 0(k1)			# k1=seg entry

	/* Validate page table pointer.  */
	beqz		k1, 3f
	nop

#ifdef __mips_n64
	MFC0		k0, MIPS_COP_0_BAD_VADDR
	PTR_SRL		k0, PDRSHIFT - PTRSHIFT		# k0=pde offset (almost)
	beq		k1, zero, MipsKernGenException	# ==0 -- no pde tab
	andi		k0, k0, PDEPTRMASK		# k0=pde offset
	PTR_ADDU	k0, k0, k1			# k0=pde entry address
	PTR_L		k1, 0(k0)			# k1=pde entry

	/* Validate pde table pointer.  */
	beqz		k1, 3f
	nop
#endif /* __mips_n64 */
	MFC0		k0, MIPS_COP_0_BAD_VADDR	# k0=bad address (again)
	PTR_SRL		k0, PAGE_SHIFT - PTESHIFT	# k0=VPN
	andi		k0, k0, PTEMASK			# k0=page tab offset
	PTR_ADDU	k1, k1, k0			# k1=pte address
	PTE_L		k0, 0(k1)			# k0=this PTE

	/* Validate page table entry.  */
	andi		k0, PTE_V
	beqz		k0, 3f
	nop

	/* Check whether this is an even or odd entry.  */
	andi		k0, k1, PTESIZE
	bnez		k0, odd_page
	nop

	PTE_L		k0, 0(k1)

	CLEAR_PTE_SWBITS(k0)
	PTE_MTC0	k0, MIPS_COP_0_TLB_LO0
	COP0_SYNC

	PTE_L		k0, PTESIZE(k1)

	CLEAR_PTE_SWBITS(k0)
	PTE_MTC0	k0, MIPS_COP_0_TLB_LO1
	COP0_SYNC

	b		tlb_insert_entry
	nop

odd_page:
	PTE_L		k0, -PTESIZE(k1)

	CLEAR_PTE_SWBITS(k0)
	PTE_MTC0	k0, MIPS_COP_0_TLB_LO0
	COP0_SYNC

	PTE_L		k0, 0(k1)

	CLEAR_PTE_SWBITS(k0)
	PTE_MTC0	k0, MIPS_COP_0_TLB_LO1
	COP0_SYNC

tlb_insert_entry:
	tlbp
	HAZARD_DELAY
	mfc0		k0, MIPS_COP_0_TLB_INDEX
	bltz		k0, tlb_insert_random
	nop
	tlbwi
	PTE_MTC0	zero, MIPS_COP_0_TLB_PG_MASK
	COP0_SYNC
#ifdef CPU_CHERI
	CHERI_EXCEPTION_RETURN(k0)
#endif
	eret
	ssnop

tlb_insert_random:
	tlbwr
	PTE_MTC0	zero, MIPS_COP_0_TLB_PG_MASK
	COP0_SYNC
#ifdef CPU_CHERI
	CHERI_EXCEPTION_RETURN(k0)
#endif
	eret
	ssnop

3:
	/*
	 * Branch to the comprehensive exception processing.
	 */
	mfc0	k1, MIPS_COP_0_STATUS
	andi	k1, k1, MIPS_SR_KSU_USER
	bnez	k1, _C_LABEL(MipsUserGenException)
	nop

	/*
	 * Check for kernel stack overflow.
	 */
	GET_CPU_PCPU(k1)
	PTR_L	k0, PC_CURTHREAD(k1)
	PTR_L	k0, TD_KSTACK(k0)
	sltu	k0, k0, sp
	bnez	k0, _C_LABEL(MipsKernGenException)
	nop

	/*
	 * Kernel stack overflow.
	 *
	 * Move to a valid stack before we call panic. We use the boot stack
	 * for this purpose.
	 */
	GET_CPU_PCPU(k1)
	lw	k1, PC_CPUID(k1)
	sll	k1, k1, PAGE_SHIFT + 1

	PTR_LA	k0, _C_LABEL(pcpu_space)
	PTR_ADDU	k0, PAGE_SIZE * 2
	PTR_ADDU	k0, k0, k1

	/*
	 * Convergence of interests: save the original 'sp' in 'k1' so that
	 * SAVE_CPU can preserve that rather than our modified 'sp' used to
	 * access the stack during the exception handler.  But also save it so
	 * that we can restore it later.
	 */
	move	k1, sp

	move	sp, k0
	PTR_SUBU	sp, sp, KERN_EXC_FRAME_SIZE
	CHERI_ADJUST_SP(sp)

	move	k0, ra
	move	ra, zero
	REG_S	ra, CALLFRAME_RA(sp)	/* stop the ddb backtrace right here */
	REG_S	zero, CALLFRAME_SP(sp)
	move	ra, k0

	/*
	 * Save CPU state, building 'frame'.  sp holds the location to save
	 * the trap frame, whereas k1 holds the value of sp to save in the
	 * trap frame.
	 */
	SAVE_CPU

	/*
	 * Now restore the value of 'sp' at the time of the tlb exception in
	 * the trapframe.
	 */
	SAVE_REG(k1, SP, sp)

	/*
	 * Squelch any more overflow checks by setting the stack base to 0.
	 */
	GET_CPU_PCPU(k1)
	PTR_L	k0, PC_CURTHREAD(k1)
	PTR_S	zero, TD_KSTACK(k0)

	move	a1, a0
	PANIC("kernel stack overflow - trapframe at %p")

	/*
	 * This nop is necessary so that the 'ra' remains within the bounds
	 * of this handler. Otherwise the ddb backtrace code will think that
	 * the panic() was called from MipsTLBMissException.
	 */
	.globl	MipsKStackOverflow
MipsKStackOverflow:
	nop

	.set pop
END(MipsTLBInvalidException)

/*----------------------------------------------------------------------------
 *
 * MipsTLBMissException --
 *
 *	Handle a TLB miss exception from kernel mode in kernel space.
 *	The BaddVAddr, Context, and EntryHi registers contain the failed
 *	virtual address.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------------
 */
LEAF_NOPROFILE(MipsTLBMissException)
	.set	noat
	MFC0		k0, MIPS_COP_0_BAD_VADDR	# k0=bad address
	PTR_LI		k1, VM_MAX_KERNEL_ADDRESS	# check fault address against
	sltu		k1, k1, k0			# upper bound of kernel_segmap
	bnez		k1, MipsKernGenException	# out of bound
	lui		k1, %hi(kernel_segmap)		# k1=hi of segbase
	PTR_SRL		k0, SEGSHIFT - PTRSHIFT		# k0=seg offset (almost)
	PTR_L		k1, %lo(kernel_segmap)(k1)	# k1=segment tab base
	beq		k1, zero, MipsKernGenException	# ==0 -- no seg tab
	andi		k0, k0, PDEPTRMASK		#06: k0=seg offset
	PTR_ADDU	k1, k0, k1			# k1=seg entry address
	PTR_L		k1, 0(k1)			# k1=seg entry
	MFC0		k0, MIPS_COP_0_BAD_VADDR	# k0=bad address (again)
	beq		k1, zero, MipsKernGenException	# ==0 -- no page table
#ifdef __mips_n64
	PTR_SRL		k0, PDRSHIFT - PTRSHIFT		# k0=VPN
	andi		k0, k0, PDEPTRMASK		# k0=pde offset
	PTR_ADDU	k0, k0, k1			# k1=pde entry address
	PTR_L		k1, 0(k0)			# k1=pde entry
	MFC0		k0, MIPS_COP_0_BAD_VADDR	# k0=bad address (again)
  	beq		k1, zero, MipsKernGenException	# ==0 -- no page table
#endif
	PTR_SRL		k0, PAGE_SHIFT - PTESHIFT	# k0=VPN
	andi		k0, k0, PTE2MASK		# k0=page tab offset
	PTR_ADDU	k1, k1, k0			# k1=pte address

	PTE_L		k0, 0(k1)			# k0=lo0 pte

	# XXX Reference bit emulation

	CLEAR_PTE_SWBITS(k0)
	PTE_MTC0	k0, MIPS_COP_0_TLB_LO0		# lo0 is loaded
	COP0_SYNC

	PTE_L		k0, PTESIZE(k1)			# k0=lo1 pte

	CLEAR_PTE_SWBITS(k0)
	PTE_MTC0	k0, MIPS_COP_0_TLB_LO1		# lo1 is loaded
	COP0_SYNC
	tlbwr					# write to tlb
	HAZARD_DELAY
#ifdef CPU_CHERI
	CHERI_EXCEPTION_RETURN(k0)
#endif
	eret					# return from exception
	.set	at
END(MipsTLBMissException)

/*----------------------------------------------------------------------------
 *
 * MipsFPTrap --
 *
 *	Handle a floating point Trap.
 *
 *	MipsFPTrap(statusReg, causeReg, pc)
 *		unsigned statusReg;
 *		unsigned causeReg;
 *		unsigned pc;
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------------
 */
#if defined(CPU_HAVEFPU)
NESTED(MipsFPTrap, CALLFRAME_SIZ, ra)
	.set push
	.set hardfloat
	PTR_SUBU	sp, sp, CALLFRAME_SIZ
	mfc0	t0, MIPS_COP_0_STATUS
	HAZARD_DELAY
	REG_S	ra, CALLFRAME_RA(sp)
	.mask	0x80000000, (CALLFRAME_RA - CALLFRAME_SIZ)

#if defined(__mips_n32) || defined(__mips_n64)
	or	t1, t0, MIPS_SR_COP_1_BIT | MIPS_SR_FR
#else
	or	t1, t0, MIPS_SR_COP_1_BIT
#endif
	mtc0	t1, MIPS_COP_0_STATUS
	HAZARD_DELAY
	ITLBNOPFIX
	cfc1	t1, MIPS_FPU_CSR		# stall til FP done
	cfc1	t1, MIPS_FPU_CSR		# now get status
	nop
	sll	t2, t1, (31 - 17)		# unimplemented operation?
	bgez	t2, 3f				# no, normal trap
	nop
/*
 * We got an unimplemented operation trap so
 * fetch the instruction, compute the next PC and emulate the instruction.
 */
	bgez	a1, 1f				# Check the branch delay bit.
	nop
/*
 * The instruction is in the branch delay slot so the branch will have to
 * be emulated to get the resulting PC.
 */
	PTR_S	a2, (CALLFRAME_SIZ + 8)(sp)
	GET_CPU_PCPU(a0)
#mips64 unsafe?
	PTR_L	a0, PC_CURPCB(a0)
	PTR_ADDU a0, a0, U_PCB_REGS		# first arg is ptr to CPU registers
	move	a1, a2				# second arg is instruction PC
	move	a2, t1				# third arg is floating point CSR
	PTR_LA	t9, _C_LABEL(MipsEmulateBranch)	# compute PC after branch
	jalr	t9				# compute PC after branch
	move	a3, zero			# fourth arg is FALSE
/*
 * Now load the floating-point instruction in the branch delay slot
 * to be emulated.
 */
	PTR_L	a2, (CALLFRAME_SIZ + 8)(sp)	# restore EXC pc
	b	2f
	lw	a0, 4(a2)			# a0 = coproc instruction
/*
 * This is not in the branch delay slot so calculate the resulting
 * PC (epc + 4) into v0 and continue to MipsEmulateFP().
 */
1:
	lw	a0, 0(a2)			# a0 = coproc instruction
#xxx mips64 unsafe?
	PTR_ADDU	v0, a2, 4			# v0 = next pc
2:
	GET_CPU_PCPU(t2)
	PTR_L	t2, PC_CURPCB(t2)
	SAVE_U_PCB_REG(v0, PC, t2)		# save new pc
/*
 * Check to see if the instruction to be emulated is a floating-point
 * instruction.
 */
	srl	a3, a0, MIPS_OPCODE_SHIFT
	beq	a3, MIPS_OPCODE_C1, 4f		# this should never fail
	nop
/*
 * Send a floating point exception signal to the current process.
 */
3:
	GET_CPU_PCPU(a0)
	PTR_L	a0, PC_CURTHREAD(a0)		# get current thread
	cfc1	a2, MIPS_FPU_CSR		# code = FP execptions
	ctc1	zero, MIPS_FPU_CSR		# Clear exceptions
	PTR_LA	t9, _C_LABEL(trapsignal)
	jalr	t9
	li	a1, SIGFPE
	b	FPReturn
	nop

/*
 * Finally, we can call MipsEmulateFP() where a0 is the instruction to emulate.
 */
4:
	PTR_LA	t9, _C_LABEL(MipsEmulateFP)
	jalr	t9
	nop

/*
 * Turn off the floating point coprocessor and return.
 */
FPReturn:
	mfc0	t0, MIPS_COP_0_STATUS
	PTR_L	ra, CALLFRAME_RA(sp)
	and	t0, t0, ~MIPS_SR_COP_1_BIT
	mtc0	t0, MIPS_COP_0_STATUS
	ITLBNOPFIX
	j	ra
	PTR_ADDU sp, sp, CALLFRAME_SIZ
	.set pop
END(MipsFPTrap)
#endif /* CPU_HAVEFPU */

#ifndef INTRNG
/*
 * Interrupt counters for vmstat.
 */
	.data
	.globl intrcnt
	.globl sintrcnt
	.globl intrnames
	.globl sintrnames
intrnames:
	.space  INTRCNT_COUNT * (MAXCOMLEN + 1) * 2
sintrnames:
#ifdef __mips_n64
	.quad  INTRCNT_COUNT * (MAXCOMLEN + 1) * 2
#else
	.int  INTRCNT_COUNT * (MAXCOMLEN + 1) * 2
#endif

	.align	(_MIPS_SZLONG / 8)
intrcnt:
	.space  INTRCNT_COUNT * (_MIPS_SZLONG / 8) * 2
sintrcnt:
#ifdef __mips_n64
	.quad  INTRCNT_COUNT * (_MIPS_SZLONG / 8) * 2
#else
	.int  INTRCNT_COUNT * (_MIPS_SZLONG / 8) * 2
#endif
#endif /* INTRNG */


/*
 * Vector to real handler in KSEG1.
 */
	.text
VECTOR(MipsCache, unknown)
#ifdef CPU_CHERI
	CHERI_EXCEPTION_ENTER(k0)
#endif
	PTR_LA	k0, _C_LABEL(MipsCacheException)
	li	k1, MIPS_KSEG0_PHYS_MASK
	and	k0, k1
	PTR_LI	k1, MIPS_KSEG1_START
	or	k0, k1
	j	k0
	nop
VECTOR_END(MipsCache)

	.set	at


/*
 * Panic on cache errors.  A lot more could be done to recover
 * from some types of errors but it is tricky.
 */
NESTED_NOPROFILE(MipsCacheException, KERN_EXC_FRAME_SIZE, ra)
	.set	noat
	.mask	0x80000000, -4
	PTR_LA	k0, _C_LABEL(panic)		# return to panic
	PTR_LA	a0, 9f				# panicstr
	MFC0	a1, MIPS_COP_0_ERROR_PC
	mfc0	a2, MIPS_COP_0_CACHE_ERR	# 3rd arg cache error

	MTC0	k0, MIPS_COP_0_ERROR_PC		# set return address

	mfc0	k0, MIPS_COP_0_STATUS		# restore status
	li	k1, MIPS_SR_DIAG_PE		# ignore further errors
	or	k0, k1
	mtc0	k0, MIPS_COP_0_STATUS		# restore status
	COP0_SYNC

#ifdef CPU_CHERI
	CHERI_EXCEPTION_RETURN(k0)
#endif
	eret

	MSG("cache error @ EPC 0x%x CachErr 0x%x");
	.set	at
END(MipsCacheException)
// CHERI CHANGES START
// {
//   "updated": 20181114,
//   "target_type": "header",
//   "changes": [
//     "support"
//   ],
//   "change_comment": ""
// }
// CHERI CHANGES END
